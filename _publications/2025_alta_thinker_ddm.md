---
title: "Thinker-DDM: Modeling Deliberation for Machine Translation with a Drift-Diffusion Process"
collection: 
permalink: /publication/2025_alta_thinker_ddm
excerpt: ''
date: 2025-10-23
author: Hongbin Na, Zimu Wang, <b>Mieradilijiang Maimaiti</b>†, Tong Chen, Wei Wang, Tao Shen, and Ling Chen
conference: In Australasian Language Technology Association Workshop (ALTA) <b>(2025)</b> (†=corresponding author)
venue: ''
paperurl: 'https://openreview.net/forum?id=ncT0OvZeL6'
citation: '<br>
@inproceedings{hongbinna2025_thiner-ddm,<br>
  title={Thinker-DDM: Modeling Deliberation for Machine Translation with a Drift-Diffusion Process},<br>
  author={Hongbin Na, Zimu Wang, Mieradilijiang Maimaiti, Tong Chen, Wei Wang, Tao Shen, and Ling Chen},<br>
  booktitle={ALTA},<br>
  year={2025},<br>
}'


---
<h2><strong>Abstract</strong></h2>
Large language models (LLMs) have demonstrated promising potential in various downstream tasks, including machine translation. 
However, prior work on LLM-based machine translation has mainly focused on better utilizing training data, demonstrations, or pre-defined and universal knowledge to improve performance, with a lack of consideration of decision-making like human translators. 
In this paper, we incorporate Thinker with the Drift-Diffusion Model (Thinker-DDM) to address this issue. 
We then redefine the Drift-Diffusion process to emulate human translators' dynamic decision-making under constrained resources. 
We conduct extensive experiments under the high-resource, low-resource, and commonsense translation settings using the WMT22 and CommonMT datasets, in which Thinker-DDM outperforms baselines in the first two scenarios. 
We also perform additional analysis and evaluation on commonsense translation to illustrate the high effectiveness and efficacy of the proposed method.

\[[PDF](https://openreview.net/forum?id=ncT0OvZeL6)\]
