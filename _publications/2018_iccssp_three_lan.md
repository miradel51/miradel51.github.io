---
title: "How to Understand: Three Types of Bilingual Information Processing?"
collection: publications
permalink: /publication/2018_iccssp_three_lan
excerpt: ''
date: 2018-11-29
author: <b>Mieradilijiang Maimaiti</b>, Shunpeng Zou*, Xiaoqun Wang, Xiaohui Zou
conference: In International Conference on Cognitive Systems and Signal Processing <b>(ICCSSP-2018)</b> (*=equal contribution)
venue: ''
paperurl: 'https://link.springer.com/chapter/10.1007%2F978-981-13-7986-4_1'
citation: '<br>
@inproceedings{Maimaiti2018HowTU,<br>
  title={How to Understand: Three Types of Bilingual Information Processing?},<br>
  author={M. Maimaiti and Shunpeng Zou and Xiaoqun Wang and Xiaohui Zou},<br>
  booktitle={ICCSIP},<br>
  year={2018},<br>
}'


---
<h2><strong>Abstract</strong></h2>
Neural machine translation (NMT) has made remarkable progress in recent years, but the performance of NMT suffers from a data sparsity problem since large-scale parallel corpora are only readily available for high-resource languages (HRLs). In recent days, transfer learning (TL) has been used widely in low-resource languages (LRLs) machine translation, while TL is becoming one of the vital directions for addressing the data sparsity problem in low-resource NMT. As a solution, a transfer learning method in NMT is generally obtained via initializing the low-resource model (child) with the high-resource model (parent). However, leveraging the original TL to low-resource models is neither able to make full use of highly related multiple HRLs nor to receive different parameters from the same parents. In order to exploit multiple HRLs effectively, we present a language-independent and straightforward multi-round transfer learning (MRTL) approach to low-resource NMT. Besides, with the intention of reducing the differences between high-resource and low-resource languages at the character level, we introduce a unified transliteration method for various language families, which are both semantically and syntactically highly analogous with each other. Experiments on low-resource datasets show that our approaches are effective, significantly outperform the state-of-the-art methods, and yield improvements of up to 5.63 BLEU points.

\[[PDF](https://link.springer.com/chapter/10.1007%2F978-981-13-7986-4_1)\]  
