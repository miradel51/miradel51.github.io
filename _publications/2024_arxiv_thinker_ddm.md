---
title: "Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion Model with Large Language Models for Machine Translation"
collection: 
permalink: /publication/2024_arxiv_thinker_ddm
excerpt: ''
date: 2024-02-16
author: Hongbin Na, Zimu Wang, <b>Mieradilijiang Maimaiti</b>†, Tong Chen, Wei Wang, Tao Shen, and Ling Chen
conference: In Arxiv <b>(2024)</b> (†=corresponding author)
venue: ''
paperurl: 'https://arxiv.org/pdf/2402.10699.pdf'
citation: '<br>
@inproceedings{hongbinna2024_thiner-ddm,<br>
  title={Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion Model with Large Language Models for Machine Translation},<br>
  author={Hongbin Na, Zimu Wang, Mieradilijiang Maimaiti, Tong Chen, Wei Wang, Tao Shen, and Ling Chen},<br>
  booktitle={Arxiv},<br>
  year={2024},<br>
}'


---
<h2><strong>Abstract</strong></h2>
Large language models (LLMs) have demonstrated promising potential in various downstream tasks, including machine translation.
However, prior work on LLM-based machine translation has mainly focused on better utilizing training data, demonstrations, or predefined and universal knowledge to improve
performance, with a lack of consideration of decision-making like human translators. 
In this paper, we incorporate Thinker with the Drift-Diffusion Model (Thinker-DDM) to address this issue. 
We then redefine the Drift-Diffusion process to emulate human translators’ dynamic decision-making under constrained resources. 
We conduct extensive experiments under the high-resource, low-resource, and commonsense translation settings using the
WMT22 and CommonMT datasets, in which Thinker-DDM outperforms baselines in the first two scenarios. 
We also perform additional analysis and evaluation on commonsense translation to illustrate the high effectiveness and efficacy of the proposed method.

\[[PDF](https://arxiv.org/pdf/2402.10699.pdf)\]
